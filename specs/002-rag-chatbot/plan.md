# Implementation Plan: RAG Chatbot

**Branch**: `002-rag-chatbot` | **Date**: 2026-01-18 | **Spec**: [spec.md](./spec.md)
**Input**: Feature specification from `/specs/002-rag-chatbot/spec.md`

## Summary

Implement a RAG (Retrieval-Augmented Generation) chatbot integrated into the Physical AI textbook that enables learners to ask questions and receive contextual answers sourced from the textbook content. The chatbot will use MiniLM embeddings stored in Qdrant Cloud for retrieval and Groq API with Llama 3 for response generation, with streaming responses via Server-Sent Events.

## Technical Context

**Language/Version**: TypeScript 5.x, Node.js 20 LTS
**Primary Dependencies**: React 18 (Docusaurus), @qdrant/js-client-rest, groq-sdk, sentence-transformers (build-time)
**Storage**: Qdrant Cloud (vectors), Browser sessionStorage (conversation)
**Testing**: Manual testing (per spec: "Tests not explicitly requested")
**Target Platform**: Web (Docusaurus static site on Vercel)
**Project Type**: Web application (frontend components + serverless API)
**Performance Goals**: <5 second response time, <2 second chat widget load
**Constraints**: Free-tier infrastructure only, mobile-first design, 320px minimum viewport
**Scale/Scope**: 6 chapters (~50 vector chunks), session-based conversations

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

| Principle | Status | Evidence |
|-----------|--------|----------|
| I. Simplicity-First | ✅ PASS | Minimal dependencies, custom React widget, no heavy frameworks |
| II. Feature-Based Architecture | ✅ PASS | ChatWidget/ folder contains all chat components, hooks, and styles |
| III. RAG-Grounded Responses | ✅ PASS | All responses cite sources, MiniLM embeddings, out-of-scope handling |
| IV. Free-Tier Infrastructure | ✅ PASS | Qdrant Cloud free, Groq free, Vercel serverless free |
| V. Mobile-First Responsive | ✅ PASS | Bottom-sheet pattern, 44px touch targets, 320px minimum |
| VI. Personalization with Privacy | ✅ N/A | No personalization in MVP (session-only storage) |

**Gate Result**: PASS - All applicable principles satisfied

## Project Structure

### Documentation (this feature)

```text
specs/002-rag-chatbot/
├── plan.md              # This file
├── research.md          # Technology decisions
├── data-model.md        # Entity definitions
├── quickstart.md        # Setup instructions
├── contracts/           # TypeScript interfaces
│   └── components.ts    # Component props and API types
└── tasks.md             # Implementation tasks (generated by /sp.tasks)
```

### Source Code (repository root)

```text
docusaurus/
├── src/
│   ├── components/
│   │   └── ChatWidget/           # Chat UI components
│   │       ├── index.tsx         # Main widget export
│   │       ├── ChatButton.tsx    # Floating action button
│   │       ├── ChatPanel.tsx     # Conversation container
│   │       ├── MessageBubble.tsx # Message display
│   │       ├── ChatInput.tsx     # Text input
│   │       ├── SourceList.tsx    # Source references
│   │       ├── LoadingIndicator.tsx
│   │       ├── ErrorDisplay.tsx
│   │       └── styles.module.css # Component styles
│   ├── hooks/
│   │   ├── useChat.ts           # Chat state and API
│   │   └── useChatWidget.ts     # Widget visibility
│   ├── theme/
│   │   └── Root/index.tsx       # Widget injection (already exists)
│   └── pages/
│       └── api/
│           └── chat.ts          # Serverless chat endpoint
├── scripts/
│   └── index-content.ts         # Embedding generation
├── lib/
│   ├── qdrant.ts               # Qdrant client wrapper
│   ├── embeddings.ts           # Embedding generation
│   └── chat-service.ts         # RAG orchestration
└── .env.local                  # Environment variables
```

**Structure Decision**: Extends existing Docusaurus structure from 001-textbook-generation. All chatbot code goes in feature-based folders (ChatWidget/, hooks/). API routes use Vercel serverless convention.

## Architecture Overview

```
┌─────────────────────────────────────────────────────────────┐
│                      Browser (Client)                        │
├─────────────────────────────────────────────────────────────┤
│  ┌───────────────┐    ┌──────────────┐    ┌──────────────┐ │
│  │ ChatWidget    │───▶│ useChat hook │───▶│ EventSource  │ │
│  │ (React)       │    │ (state mgmt) │    │ (SSE stream) │ │
│  └───────────────┘    └──────────────┘    └──────────────┘ │
│          │                                       │          │
│          ▼                                       ▼          │
│  ┌───────────────┐                    ┌──────────────────┐ │
│  │sessionStorage │                    │ POST /api/chat   │ │
│  │(conversation) │                    │ (Vercel Edge)    │ │
│  └───────────────┘                    └──────────────────┘ │
└─────────────────────────────────────────────────────────────┘
                                                   │
                                                   ▼
┌─────────────────────────────────────────────────────────────┐
│                    Serverless Function                       │
├─────────────────────────────────────────────────────────────┤
│  1. Embed query (MiniLM via transformers.js)                │
│  2. Retrieve chunks (Qdrant Cloud)                          │
│  3. Generate response (Groq API + Llama 3)                  │
│  4. Stream tokens (SSE)                                     │
│  5. Return sources                                          │
└─────────────────────────────────────────────────────────────┘
                    │                    │
                    ▼                    ▼
           ┌──────────────┐    ┌──────────────────┐
           │ Qdrant Cloud │    │    Groq API      │
           │ (vectors)    │    │ (Llama 3 8B)     │
           └──────────────┘    └──────────────────┘
```

## Key Technical Decisions

| Decision | Choice | Rationale |
|----------|--------|-----------|
| Embedding model | MiniLM-L6-v2 (384 dim) | Constitution recommends, fast, free |
| Vector DB | Qdrant Cloud | Free tier, 1M vectors, REST API |
| LLM provider | Groq API | Fast inference, generous free tier |
| LLM model | Llama 3 8B | Good quality, works with Groq free |
| Streaming | Server-Sent Events | Simpler than WebSocket, one-way |
| Session storage | sessionStorage | Simplest for MVP, no backend DB |
| Embedding runtime | transformers.js | Browser-compatible, no Python needed |

## API Design

### POST /api/chat

**Request**:
```typescript
{
  query: string;           // 1-1000 chars
  conversationHistory?: Message[];  // max 10
}
```

**Response**: `text/event-stream`
```
event: token
data: {"content": "Physical"}

event: token
data: {"content": " AI"}

event: sources
data: {"citations": [{"chunkId": "ch1_sec1_0", "chapterNumber": 1, ...}]}

event: done
data: {}
```

**Error Response**:
```
event: error
data: {"message": "Rate limit exceeded", "code": "RATE_LIMIT"}
```

## Build-Time Indexing

The `scripts/index-content.ts` script runs during build to:

1. Read all `docs/*.md` files
2. Parse frontmatter for chapter/section metadata
3. Split into semantic chunks (200-300 tokens)
4. Generate MiniLM embeddings
5. Upload to Qdrant Cloud collection `textbook_chunks`

This happens once per deploy, not at runtime.

## Complexity Tracking

> No violations requiring justification. All decisions align with constitution.

| Aspect | Complexity Level | Justification |
|--------|------------------|---------------|
| Dependencies | Low | Only 3 new packages (qdrant, groq, uuid) |
| API surface | Low | Single endpoint with streaming |
| State management | Low | React useState + sessionStorage |
| Infrastructure | Low | Existing Vercel + two free APIs |

## Risk Mitigation

| Risk | Mitigation |
|------|------------|
| Groq rate limits | Queue requests, show "please wait" |
| Qdrant suspension | Weekly ping in CI/CD, fallback message |
| Slow responses (>5s) | Streaming reduces perceived latency, timeout with message |
| Poor retrieval quality | Tune chunk size, similarity threshold, top-K |

## Success Criteria Mapping

| Spec Criteria | Implementation |
|---------------|----------------|
| SC-001: <5s response | Streaming + Groq fast inference |
| SC-002: 80% source attribution | Always include top-3 chunks |
| SC-003: <2s widget load | Lazy load, minimal bundle |
| SC-004: 90% out-of-scope detection | Similarity threshold + prompt engineering |
| SC-005: <30s interaction | Optimized UX flow |
| SC-006: 320px viewport | Mobile-first CSS |

## Next Steps

1. Run `/sp.tasks` to generate implementation tasks
2. Start with Phase 1: Setup (environment, dependencies)
3. Phase 2: Build-time indexing script
4. Phase 3: API endpoint
5. Phase 4: Chat widget components
6. Phase 5: Integration and testing
